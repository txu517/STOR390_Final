{
    "collab_server" : "",
    "contents" : "\n# helper functions and libraries\n\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(GGally)\n\n# compute the cosine similarity between vectors 'A', 'B'\ncosine_similarity <- function(A, B)\n{\n  ip = t(A) %*% B\n  lA = sqrt(sum(A^2))\n  lB = sqrt(sum(B^2))\n  \n  return(ip / (lA * lB))\n}\n\n# compute the cosine similarity matrix between all columns of a matrix/df 'col_ordered_vectors'\ncosine_similarity_matrix <- function(col_ordered_vectors)\n{\n  n = ncol(col_ordered_vectors)\n  df = matrix(0, nrow = n, ncol = n)\n  for(i in 1:n)\n  {\n    for(j in 1:n)\n    {\n      df[i,j] = cosine_similarity(col_ordered_vectors[,i], col_ordered_vectors[,j])\n    }\n  }\n  \n  return(df)\n}\n\n# create a df of non-stop word frequencies for each book id in 'book_ids' and filter out words\n# that are not shared by at least 'keep_threshold' percent of the books, optional 'mirror' parameter\n# for downloading book tokens\nget_word_freq_vectors <- function(book_ids, keep_threshold, mirror = 'http://mirrors.xmission.com/gutenberg/')\n{\n  first = book_ids[1]\n  others = book_ids[-1]\n  \n  totalBookCount = length(book_ids)\n  \n  result = gutenberg_download(first, mirror) %>% \n    unnest_tokens(word, text) %>% \n    anti_join(stop_words) %>% \n    count(word)\n  \n  colnames(result)[2] = paste0(\"id_\", first)\n  \n  cnt = 0\n  for(i in others)\n  {\n    book = gutenberg_download(i, mirror)\n    book = book %>% \n      unnest_tokens(word, text) %>% \n      anti_join(stop_words) %>% \n      count(word)\n    colnames(book)[2] = paste0(\"id_\", i)\n    result = full_join(book, result, by = \"word\")\n    cnt = cnt + 1\n    message(paste0(\"finished \", cnt, \" / \", totalBookCount))\n  }\n  \n  freq = result[,2:ncol(result)]\n  \n  freq[!is.na(freq)] = T\n  \n  return(result)\n}\n\n# get subjects for all books in gutenberg repo\nget_all_book_subjects <- function()\n{\n  filt = gutenberg_subjects %>% \n    separate(subject, paste0('c',as.character(1:10)), sep = \"\\\\s+--\\\\s+\") %>%\n    gather(num, subject, c1:c10) %>%\n    filter(!is.na(subject), subject_type != 'lcc') %>%\n    select(-num)\n  \n  filt$subject = str_to_lower(filt$subject)\n  \n  filt = unique(filt)\n  \n  return(filt)\n}\n\n# get all unique authors in the gutenberg repo\nget_all_authors <- function()\n{\n  return(gutenberg_authors %>% unique)\n}\n\n# retreive all filtered works\nget_all_works <- function()\n{\n  return(gutenberg_works() %>% filter(!is.na(title) & !is.na(author) & str_detect(rights, 'Public domain')))\n}\n\n# filter out non-english, non-unique, non-public, empty book ids from a vector of 'book_ids'\nfilter_book_ids <- function(book_ids)\n{\n  keep_ids = get_all_works()$gutenberg_id\n  \n  return(book_ids[book_ids %in% keep_ids])\n}\n\n# helper column sample variance function\ncolVariance <- function(m)\n{\n  cm = colMeans(m)\n  cv = 1:ncol(m)\n  for(i in 1:ncol(m))\n  {\n    cv[i] = 0\n    for(j in 1:nrow(m))\n    {\n      cv[i] = cv[i] + (m[j,i] - cm[i])^2\n    }\n    cv[i] = cv[i] / (nrow(m) - 1)\n  }\n  \n  return(cv)\n}\n\nsubjects <- get_all_book_subjects() %>% group_by(subject) %>% summarise(freq = n()) %>% arrange(desc(freq))\n\ntopSubjects <- subjects[1:20,]\n\ntopSubjects$subject <- factor(topSubjects$subject, levels = topSubjects$subject)\n\nggplot(data = topSubjects) +\n  geom_bar(aes(x = subject, weight = freq)) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\nfictions <- subjects %>% filter(str_detect(subject, \"fiction\") & subject != \"fiction\")\n\n# total sub-genres\nnrow(fictions)\n\ntopCnt <- 8;\n\nothercnt <- sum(fictions$freq) - sum(top_n(fictions, topCnt)$freq)\n\nfreqVec <- c(top_n(fictions, topCnt)$freq, othercnt)\nnameVec <- c(top_n(fictions, topCnt)$subject, 'others')\n\ntibble() %>% ggplot(aes(x=\"\", y=freqVec, fill=nameVec)) + geom_bar(width = 1, stat = \"identity\") + coord_polar(\"y\", start=0) + theme(axis.text.x=element_blank()) + labs(x='',y='')\n\nauthorCounts <- get_all_works() %>% \n  group_by(gutenberg_author_id) %>% \n  summarise(booksWritten = n()) %>% \n  inner_join(get_all_authors(), by = \"gutenberg_author_id\") %>% \n  select(author, booksWritten) %>%\n  arrange(desc(booksWritten))\n\nauthorCounts\n\nset.seed(2321)\n\nworks <- get_all_works()\nworks <- works[sample(1:nrow(works), 10),]\n\nget_word_freq_vectors(works$gutenberg_id, 0.8)\n\ncounts <- 1:10\n\nfor(i in 1:10)\n{\n  counts[i] <- nrow(get_word_freq_vectors(works$gutenberg_id, i / 10))\n}\n\ntibble() %>% ggplot() + geom_point(aes(x=(1:10)/10,y=counts)) + geom_line(aes(x=(1:10)/10,y=counts)) + labs(x=\"threshold\",y=\"word count\")\n\nset.seed(2532)\nbooks <- (get_all_book_subjects() %>% filter(subject == \"fiction\"))$gutenberg_id\nbooks <- filter_book_ids(books)\nbooks <- books[sample(1:length(books), 6)]\nbooks\nwordFreqs <- get_word_freq_vectors(books, 0.9)\nView(wordFreqs)\n\n# number of words\nnrow(wordFreqs)\n\n# number of books\nncol(wordFreqs) - 1\n\nvec <- t(data.matrix(wordFreqs[,2:ncol(wordFreqs)]))\nvecMeans <- colMeans(vec)\nvecVar <- colVariance(vec)\n\n# standardize by subtracting mean and dividing by column std\nfor(i in 1:nrow(vec))\n{\n  vec[i,] = (vec[i,] - vecMeans) / sqrt(vecVar)\n}\n\n# perform SVD on the standardized data\nvecSVD <- svd(vec, nu = 0, nv = 20)\n\n# transform the data into the new domain spanned by the first 20 principle vectors\nvec <- vec %*% vecSVD$v\n\nA <- cosine_similarity_matrix(t(vec)[,1:20])\n\nsimMat <- simMat + 1\nnet = network(simMat, directed = FALSE, ignore.eval = FALSE, names.eval = \"weights\")\nggnet2(net, label = colnames(temp)[2:11], edge.size = \"weights\")\n\n",
    "created" : 1492636082959.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3623172689",
    "id" : "ABF96319",
    "lastKnownWriteTime" : 1492636066,
    "last_content_update" : 1492648364241,
    "path" : "~/Downloads/finalproj.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}