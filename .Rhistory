count(word)
colnames(book)[2] <- i
result <- full_join(book, result, by = "word")
}
result[result[,-word] > 0] <- TRUE
result[is.na(result)] <- FALSE
return(result)
}
temp <- getbestwords(c(2,3), mirror)
ncol(result)
ncol(temp)
getbestwords <- function(idvec, mirror = 'http://mirrors.xmission.com/gutenberg/'){
first <- idvec[1]
others <- idvec[-1]
result <- gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] <- first
for(i in others){
book = gutenberg_download(i, mirror)
book <- book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] <- i
result <- full_join(book, result, by = "word")
}
result[result[2:ncol(result)] > 0] <- TRUE
result[is.na(result)] <- FALSE
return(result)
}
temp <- getbestwords(c(2,3), mirror)
getbestwords <- function(idvec, mirror = 'http://mirrors.xmission.com/gutenberg/'){
first <- idvec[1]
others <- idvec[-1]
result <- gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] <- first
for(i in others){
book = gutenberg_download(i, mirror)
book <- book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] <- i
result <- full_join(book, result, by = "word")
}
result[result[,2:ncol(result)] > 0] <- TRUE
result[is.na(result)] <- FALSE
return(result)
}
temp <- getbestwords(c(2,3), mirror)
getwordfreq(filt$gutenberg_id)
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt) %>% filter(subject == 'fiction')
filt <- filt[1:10, ]
getwordfreq(filt$gutenberg_id)
print(5)
library(gutenbergr)
library(tidyverse)
library(stringr)
library(tidytext)
library(GGally)
cosSimilarity <- function(A, B)
{
ip = t(A) %*% B
lA = sqrt(sum(A^2))
lB = sqrt(sum(B^2))
return(ip / (lA * lB))
}
cosSimilarityMatrix <- function(colOrderDF)
{
n = ncol(colOrderDF)
df <- matrix(0, nrow = n, ncol = n)
for(i in 1:n)
{
for(j in 1:n)
{
df[i,j] = cosSimilarity(colOrderDF[,i], colOrderDF[,j])
}
}
return(df)
}
getwordfreq <- function(idvec, mirror = 'http://mirrors.xmission.com/gutenberg/'){
first <- idvec[1]
others <- idvec[-1]
totalBookCount = length(idvec)
result <- gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] = paste0("c", first)
cnt = 0
for(i in others){
book = gutenberg_download(i, mirror)
book <- book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] = paste0("c", i)
result <- full_join(book, result, by = "word")
cnt = cnt + 1
print(paste0("finished ", cnt, " / ", totalBookCount))
}
freq <- result[,2:ncol(result)]
freq[!is.na(freq)] = T
freq[is.na(freq)] = F
freq <- rowSums(freq)
result[is.na(result)] = 0
result <- result %>% add_column(rs = freq) %>% filter(rs >= totalBookCount * 0.75) %>% select(-rs)
return(result)
}
meta <- gutenberg_works()
meta <- filter(meta, !is.na(title) & !is.na(author) & str_detect(rights, 'Public domain'))
#meta <- meta[sample(1:nrow(meta), 50), ]
View(meta)
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt)
filt <- filter(filt, subject == 'fiction')
filt <- filt[1:350,]
temp <- getwordfreq(filt$gutenberg_id)
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt)
filt <- filter(filt, subject == 'fiction')
ggplot(data = subjCounts) +
geom_bar(aes(x = subject, weight = freq)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
subjCounts <- group_by(filt, subject) %>% summarise(freq = n()) %>% arrange(desc(freq))
subjCounts <- subjCounts[1:20,]
subjCounts$subject <- factor(subjCounts$subject, levels = subjCounts$subject)
ggplot(data = subjCounts) +
geom_bar(aes(x = subject, weight = freq)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt)
subjCounts <- group_by(filt, subject) %>% summarise(freq = n()) %>% arrange(desc(freq))
subjCounts <- subjCounts[1:20,]
subjCounts$subject <- factor(subjCounts$subject, levels = subjCounts$subject)
ggplot(data = subjCounts) +
geom_bar(aes(x = subject, weight = freq)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
subjects <- subjCounts %>%
filter(str_detect(subject, 'fiction') & subject != 'fiction')
View(subjects)
subjCounts <- group_by(filt, subject) %>% summarise(freq = n()) %>% arrange(desc(freq))
subjects <- subjCounts %>%
filter(str_detect(subject, 'fiction') & subject != 'fiction')
gutenberg_authors
?sample()
getwordfreq <- function(idvec, mirror = 'http://mirrors.xmission.com/gutenberg/'){
first <- idvec[1]
others <- idvec[-1]
totalBookCount = length(idvec)
result <- gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] = paste0("c", first)
cnt = 0
for(i in others){
book = gutenberg_download(i, mirror)
book <- book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] = paste0("c", i)
result <- full_join(book, result, by = "word")
cnt = cnt + 1
print(paste0("finished ", cnt, " / ", totalBookCount))
}
freq <- result[,2:ncol(result)]
freq[!is.na(freq)] = T
freq[is.na(freq)] = F
freq <- rowSums(freq)
result[is.na(result)] = 0
result <- result %>% add_column(rs = freq) %>% filter(rs >= totalBookCount * 0.75) %>% select(-rs)
return(result)
}
temp <- getwordfreq(filt$gutenberg_id)
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt)
filt <- filter(filt, subject == 'fiction')
filt <- filt[1:350,]
temp <- getwordfreq(filt$gutenberg_id)
filt <- filt[1:5,]
temp <- getwordfreq(filt$gutenberg_id)
View(temp)
get_word_freq_vectors <- function(book_ids, keep_threshold, mirror = 'http://mirrors.xmission.com/gutenberg/')
{
first = book_ids[1]
others = book_ids[-1]
totalBookCount = length(book_ids)
result = gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] = paste0("id_", first)
cnt = 0
for(i in others)
{
book = gutenberg_download(i, mirror)
book = book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] = paste0("id_", i)
result = full_join(book, result, by = "word")
cnt = cnt + 1
message(paste0("finished ", cnt, " / ", totalBookCount))
}
freq = result[,2:ncol(result)]
freq[!is.na(freq)] = T
freq[is.na(freq)] = F
freq <- rowSums(freq)
result[is.na(result)] = 0
result <- result %>% add_column(rs = freq) %>% filter(rs >= totalBookCount * 0.75) %>% select(-rs)
return(result)
return(result)
}
set.seed(2532)
books <- (get_all_book_subjects() %>% filter(subject == "fiction"))$gutenberg_id
cosine_similarity <- function(A, B)
{
ip = t(A) %*% B
lA = sqrt(sum(A^2))
lB = sqrt(sum(B^2))
return(ip / (lA * lB))
}
cosine_similarity_matrix <- function(col_ordered_vectors)
{
n = ncol(col_ordered_vectors)
df = matrix(0, nrow = n, ncol = n)
for(i in 1:n)
{
for(j in 1:n)
{
df[i,j] = cosine_similarity(col_ordered_vectors[,i], col_ordered_vectors[,j])
}
}
return(df)
}
get_word_freq_vectors <- function(book_ids, keep_threshold, mirror = 'http://mirrors.xmission.com/gutenberg/')
{
first = book_ids[1]
others = book_ids[-1]
totalBookCount = length(book_ids)
result = gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] = paste0("id_", first)
cnt = 0
for(i in others)
{
book = gutenberg_download(i, mirror)
book = book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] = paste0("id_", i)
result = full_join(book, result, by = "word")
cnt = cnt + 1
message(paste0("finished ", cnt, " / ", totalBookCount))
}
freq = result[,2:ncol(result)]
freq[!is.na(freq)] = T
freq[is.na(freq)] = F
freq <- rowSums(freq)
result[is.na(result)] = 0
result <- result %>% add_column(rs = freq) %>% filter(rs >= totalBookCount * 0.75) %>% select(-rs)
return(result)
return(result)
}
get_all_book_subjects <- function()
{
filt = gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject = str_to_lower(filt$subject)
filt = unique(filt)
return(filt)
}
get_all_authors <- function()
{
return(gutenberg_authors %>% unique)
}
get_all_works <- function()
{
return(gutenberg_works() %>% filter(!is.na(title) & !is.na(author) & str_detect(rights, 'Public domain')))
}
filter_book_ids <- function(book_ids)
{
keep_ids = get_all_works()$gutenberg_id
return(book_ids[book_ids %in% keep_ids])
}
colVariance <- function(m)
{
cm = colMeans(m)
cv = 1:ncol(m)
for(i in 1:ncol(m))
{
cv[i] = 0
for(j in 1:nrow(m))
{
cv[i] = cv[i] + (m[j,i] - cm[i])^2
}
cv[i] = cv[i] / (nrow(m) - 1)
}
return(cv)
}
set.seed(2532)
books <- (get_all_book_subjects() %>% filter(subject == "fiction"))$gutenberg_id
books
books <- filter_book_ids(books)
books <- books[sample(1:length(books), 6)]
books
wordFreqs <- get_word_freq_vectors(books, 0.9)
View(wordFreqs)
simMat <- cosSimilarityMatrix(t(transVecs)[,1:10])
transVecs <- origVecs %*% origVecsSvd$v
origVecs <- t(data.matrix(temp[,2:ncol(temp)]))
vecMeans <- colMeans(origVecs)
for(i in 1:nrow(origVecs))
{
origVecs[i,] = origVecs[i,] - vecMeans
}
View(origVecs)
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
transVecs <- origVecs %*% origVecsSvd$v
View(transVecs)
pcaframe <- as.data.frame(transVecs)
ggplot(data = pcaframe) + geom_point(aes(x = V1, y = V2))
simMat <- cosSimilarityMatrix(t(transVecs)[,1:10])
filt <- gutenberg_subjects %>%
separate(subject, paste0('c',as.character(1:10)), sep = "\\s+--\\s+") %>%
gather(num, subject, c1:c10) %>%
filter(!is.na(subject), subject_type != 'lcc') %>%
select(-num)
filt$subject <- str_to_lower(filt$subject)
filt <- unique(filt)
filt <- filter(filt, subject == 'fiction')
filt <- filt[1:20,]
temp <- getwordfreq(filt$gutenberg_id)
View(temp)
origVecs <- t(data.matrix(temp[,2:ncol(temp)]))
vecMeans <- colMeans(origVecs)
for(i in 1:nrow(origVecs))
{
origVecs[i,] = origVecs[i,] - vecMeans
}
View(origVecs)
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
transVecs <- origVecs %*% origVecsSvd$v
View(transVecs)
pcaframe <- as.data.frame(transVecs)
ggplot(data = pcaframe) + geom_point(aes(x = V1, y = V2))
simMat <- cosSimilarityMatrix(t(transVecs)[,1:10])
View(simMat)
simMat <- simMat + 1
net = network(simMat, directed = FALSE, ignore.eval = FALSE, names.eval = "weights")
library(gutenbergr)
library(tidyverse)
library(stringr)
library(tidytext)
library(GGally)
?NETWORK()
library(network)
install.packages('netowkr')
install.packages('network')
net = network(simMat, directed = FALSE, ignore.eval = FALSE, names.eval = "weights")
library(network)
net = network(simMat, directed = FALSE, ignore.eval = FALSE, names.eval = "weights")
ggnet2(net, label = colnames(temp)[2:11], edge.size = "weights")
library(ggnet)
library(ggnetw)
library(ggnet2)
library(network)
ggnet2(net, label = colnames(temp)[2:11], edge.size = "weights")
install.packages(ggnet2)
install.packages('ggnet')
install.packages('ggally')
y
install.packages("GGally")
library(GGally)
ggnet2(net, label = colnames(temp)[2:11], edge.size = "weights")
net = network(simMat, directed = FALSE, ignore.eval = FALSE, names.eval = "weights")
ggnet2(net, label = colnames(temp)[2:11], edge.size = "weights")
install.packages('sna')
library(sna)
ggnet2(net, label = colnames(temp)[2:11], edge.size = "weights")
library(GGally)
library(gutenbergr)
library(tidyverse)
library(stringr)
library(tidytext)
library(GGally)
cosSimilarity <- function(A, B)
{
ip = t(A) %*% B
lA = sqrt(sum(A^2))
lB = sqrt(sum(B^2))
return(ip / (lA * lB))
}
cosSimilarityMatrix <- function(colOrderDF)
{
n = ncol(colOrderDF)
df <- matrix(0, nrow = n, ncol = n)
for(i in 1:n)
{
for(j in 1:n)
{
df[i,j] = cosSimilarity(colOrderDF[,i], colOrderDF[,j])
}
}
return(df)
}
getwordfreq <- function(idvec, mirror = 'http://mirrors.xmission.com/gutenberg/'){
first <- idvec[1]
others <- idvec[-1]
totalBookCount = length(idvec)
result <- gutenberg_download(first, mirror) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(result)[2] = paste0("c", first)
cnt = 0
for(i in others){
book = gutenberg_download(i, mirror)
book <- book %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word)
colnames(book)[2] = paste0("c", i)
result <- full_join(book, result, by = "word")
cnt = cnt + 1
print(paste0("finished ", cnt, " / ", totalBookCount))
}
freq <- result[,2:ncol(result)]
freq[!is.na(freq)] = T
freq[is.na(freq)] = F
freq <- rowSums(freq)
result[is.na(result)] = 0
result <- result %>% add_column(rs = freq) %>% filter(rs >= totalBookCount * 0.75) %>% select(-rs)
return(result)
}
temp <- getwordfreq(c(1,2,3,5))
View(temp)
simMat <- cosSimilarityMatrix(t(transVecs)[,1:10])
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
origVecs <- t(data.matrix(temp[,2:ncol(temp)]))
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
transVecs <- origVecs %*% origVecsSvd$v
View(transVecs)
simMat <- cosSimilarityMatrix(t(transVecs)[,1:10])
simMat <- cosSimilarityMatrix(t(transVecs)[,1:4])
View(simMat)
temp <- getwordfreq(c(1,2,3,5,10,14,15))
origVecs <- t(data.matrix(temp[,2:ncol(temp)]))
vecMeans <- colMeans(origVecs)
for(i in 1:nrow(origVecs))
{
origVecs[i,] = origVecs[i,] - vecMeans
}
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
transVecs <- origVecs %*% origVecsSvd$v
simMat <- cosSimilarityMatrix(t(transVecs)[,1:4])
View(simMat)
origVecs <- t(data.matrix(temp[,2:ncol(temp)]))
vecMeans <- colMeans(origVecs)
for(i in 1:nrow(origVecs))
{
origVecs[i,] = origVecs[i,] - vecMeans
}
origVecsSvd <- svd(origVecs, nu = 0, nv = 50)
transVecs <- origVecs %*% origVecsSvd$v
simMat <- cosSimilarityMatrix(t(transVecs)[,1:7])
View(simMat)
